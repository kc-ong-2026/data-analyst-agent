# Application Configuration

app:
  name: "Govtech Chat Assistant"
  version: "1.0.0"
  debug: false

# LLM Configuration - Model agnostic setup
llm:
  # Default provider to use when not specified
  default_provider: anthropic

  # Available providers: openai, anthropic, google
  providers:
    openai:
      models:
        - gpt-4-turbo-preview
        - gpt-4
        - gpt-3.5-turbo
      default_model: gpt-4-turbo-preview
      temperature: 0.7
      max_tokens: 4096

    anthropic:
      models:
        - claude-opus-4-6  # Claude Opus 4.6 (latest, most capable)
        - claude-sonnet-4-5-20250929
        - claude-3-opus-20240229
        - claude-3-sonnet-20240229
        - claude-3-haiku-20240307
      default_model: claude-sonnet-4-5-20250929
      temperature: 0.7
      max_tokens: 4096

      # Analytics Agent specific configuration
      analytics_model: claude-opus-4-6  # Use Opus 4.6 for analytics
      analytics_temperature: 0.0  # Lower temp for more deterministic code generation
      analytics_max_tokens: 8192  # More tokens for chain of thought reasoning

    google:
      models:
        - gemini-pro
        - gemini-pro-vision
      default_model: gemini-pro
      temperature: 0.7
      max_tokens: 4096

# Embedding Configuration
embeddings:
  # Default provider to use when not specified
  default_provider: openai

  providers:
    openai:
      models:
        - text-embedding-3-small
        - text-embedding-3-large
        - text-embedding-ada-002
      default_model: text-embedding-3-small
      dimensions: 1536

    google:
      models:
        - models/embedding-001
      default_model: models/embedding-001
      dimensions: 768

# Vector Store Configuration
vector_store:
  type: pgvector  # Options: chroma, faiss, pgvector
  persist_directory: "./data/vector_store"
  collection_name: "govtech_data"

# Database Configuration
database:
  pool_size: 5
  max_overflow: 10
  echo: false

# RAG Configuration
rag:
  embedding_batch_size: 100
  vector_search_top_k: 20
  fulltext_search_top_k: 20
  hybrid_top_k: 10
  rrf_k: 60
  similarity_threshold: 0.8

  # Reranking Configuration
  use_reranking: true                          # Enable cross-encoder reranking
  reranker_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"  # HuggingFace model
  reranker_batch_size: 32                      # Batch size for reranking

  # BM25 Search Configuration
  use_bm25: true                               # Use BM25 instead of PostgreSQL full-text

  # Dataset Selection Configuration
  confidence_threshold: 0.5                    # Minimum score to load dataset (0-1)
  min_datasets: 1                              # Always load at least this many datasets
  max_datasets: 3                              # Maximum datasets to load

# Data Configuration
data:
  datasets_path: "dataset"
  api_specs_path: "api_spec"

# LangGraph Configuration
langgraph:
  max_iterations: 10
  recursion_limit: 25

# LangSmith Configuration
langsmith:
  enabled: true  # Master switch for tracing
  project_name: "govtech-multi-agent-system"
  tags:
    - "production"
    - "multi-agent"
    - "singapore-data"
