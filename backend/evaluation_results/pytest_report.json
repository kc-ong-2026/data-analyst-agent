{"created": 1770835636.6965473, "duration": 809.9687740802765, "exitcode": 2, "root": "/app", "environment": {}, "summary": {"passed": 1, "total": 1, "collected": 35}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests/evaluation", "type": "Package"}]}, {"nodeid": "tests/evaluation/test_generation_metrics.py::TestBERTScoreEvaluation", "outcome": "passed", "result": [{"nodeid": "tests/evaluation/test_generation_metrics.py::TestBERTScoreEvaluation::test_bertscore_on_generated_answers", "type": "Coroutine", "lineno": 25}, {"nodeid": "tests/evaluation/test_generation_metrics.py::TestBERTScoreEvaluation::test_bertscore_per_agent", "type": "Coroutine", "lineno": 79}, {"nodeid": "tests/evaluation/test_generation_metrics.py::TestBERTScoreEvaluation::test_bertscore_with_threshold_check", "type": "Coroutine", "lineno": 103}]}, {"nodeid": "tests/evaluation/test_generation_metrics.py::TestFaithfulness", "outcome": "passed", "result": [{"nodeid": "tests/evaluation/test_generation_metrics.py::TestFaithfulness::test_answers_grounded_in_context", "type": "Coroutine", "lineno": 130}, {"nodeid": "tests/evaluation/test_generation_metrics.py::TestFaithfulness::test_faithfulness_detects_hallucination", "type": "Coroutine", "lineno": 184}]}, {"nodeid": "tests/evaluation/test_generation_metrics.py::TestAnswerRelevancy", "outcome": "passed", "result": [{"nodeid": "tests/evaluation/test_generation_metrics.py::TestAnswerRelevancy::test_answers_address_query", "type": "Coroutine", "lineno": 231}]}, {"nodeid": "tests/evaluation/test_generation_metrics.py::TestAnswerCorrectness", "outcome": "passed", "result": [{"nodeid": "tests/evaluation/test_generation_metrics.py::TestAnswerCorrectness::test_factual_accuracy", "type": "Coroutine", "lineno": 289}]}, {"nodeid": "tests/evaluation/test_generation_metrics.py::TestAgentSpecificMetrics", "outcome": "passed", "result": [{"nodeid": "tests/evaluation/test_generation_metrics.py::TestAgentSpecificMetrics::test_verification_agent_accuracy", "type": "Coroutine", "lineno": 348}, {"nodeid": "tests/evaluation/test_generation_metrics.py::TestAgentSpecificMetrics::test_extraction_agent_data_relevance", "type": "Coroutine", "lineno": 387}, {"nodeid": "tests/evaluation/test_generation_metrics.py::TestAgentSpecificMetrics::test_analytics_answer_quality", "type": "Coroutine", "lineno": 436}]}, {"nodeid": "tests/evaluation/test_generation_metrics.py::TestEndToEndGeneration", "outcome": "passed", "result": [{"nodeid": "tests/evaluation/test_generation_metrics.py::TestEndToEndGeneration::test_full_pipeline_generation_metrics", "type": "Coroutine", "lineno": 485}]}, {"nodeid": "tests/evaluation/test_generation_metrics.py", "outcome": "passed", "result": [{"nodeid": "tests/evaluation/test_generation_metrics.py::TestBERTScoreEvaluation", "type": "Class"}, {"nodeid": "tests/evaluation/test_generation_metrics.py::TestFaithfulness", "type": "Class"}, {"nodeid": "tests/evaluation/test_generation_metrics.py::TestAnswerRelevancy", "type": "Class"}, {"nodeid": "tests/evaluation/test_generation_metrics.py::TestAnswerCorrectness", "type": "Class"}, {"nodeid": "tests/evaluation/test_generation_metrics.py::TestAgentSpecificMetrics", "type": "Class"}, {"nodeid": "tests/evaluation/test_generation_metrics.py::TestEndToEndGeneration", "type": "Class"}]}, {"nodeid": "tests/evaluation/test_llm_as_judge.py::TestLLMJudgeAccuracy", "outcome": "passed", "result": [{"nodeid": "tests/evaluation/test_llm_as_judge.py::TestLLMJudgeAccuracy::test_judge_evaluates_analytics_responses", "type": "Coroutine", "lineno": 24}, {"nodeid": "tests/evaluation/test_llm_as_judge.py::TestLLMJudgeAccuracy::test_judge_evaluates_code_quality", "type": "Coroutine", "lineno": 89}, {"nodeid": "tests/evaluation/test_llm_as_judge.py::TestLLMJudgeAccuracy::test_judge_evaluates_explanation_quality", "type": "Coroutine", "lineno": 137}]}, {"nodeid": "tests/evaluation/test_llm_as_judge.py::TestJudgeConsistency", "outcome": "passed", "result": [{"nodeid": "tests/evaluation/test_llm_as_judge.py::TestJudgeConsistency::test_judge_gives_consistent_scores", "type": "Coroutine", "lineno": 185}, {"nodeid": "tests/evaluation/test_llm_as_judge.py::TestJudgeConsistency::test_judge_consistency_across_criteria", "type": "Coroutine", "lineno": 214}]}, {"nodeid": "tests/evaluation/test_llm_as_judge.py::TestJudgeDiscrimination", "outcome": "passed", "result": [{"nodeid": "tests/evaluation/test_llm_as_judge.py::TestJudgeDiscrimination::test_judge_distinguishes_good_vs_poor_responses", "type": "Coroutine", "lineno": 254}, {"nodeid": "tests/evaluation/test_llm_as_judge.py::TestJudgeDiscrimination::test_judge_detects_inaccuracies", "type": "Coroutine", "lineno": 293}, {"nodeid": "tests/evaluation/test_llm_as_judge.py::TestJudgeDiscrimination::test_judge_detects_incomplete_answers", "type": "Coroutine", "lineno": 327}]}, {"nodeid": "tests/evaluation/test_llm_as_judge.py::TestJudgePairwiseComparison", "outcome": "passed", "result": [{"nodeid": "tests/evaluation/test_llm_as_judge.py::TestJudgePairwiseComparison::test_pairwise_comparison", "type": "Coroutine", "lineno": 371}, {"nodeid": "tests/evaluation/test_llm_as_judge.py::TestJudgePairwiseComparison::test_comparison_prefers_more_complete_answer", "type": "Coroutine", "lineno": 395}]}, {"nodeid": "tests/evaluation/test_llm_as_judge.py::TestJudgeWithGroundTruth", "outcome": "passed", "result": [{"nodeid": "tests/evaluation/test_llm_as_judge.py::TestJudgeWithGroundTruth::test_judge_aligns_with_ground_truth", "type": "Coroutine", "lineno": 428}]}, {"nodeid": "tests/evaluation/test_llm_as_judge.py::TestJudgeThresholds", "outcome": "passed", "result": [{"nodeid": "tests/evaluation/test_llm_as_judge.py::TestJudgeThresholds::test_responses_meet_quality_thresholds", "type": "Coroutine", "lineno": 483}]}, {"nodeid": "tests/evaluation/test_llm_as_judge.py::TestJudgeCriteria", "outcome": "passed", "result": [{"nodeid": "tests/evaluation/test_llm_as_judge.py::TestJudgeCriteria::test_accuracy_criterion", "type": "Coroutine", "lineno": 552}, {"nodeid": "tests/evaluation/test_llm_as_judge.py::TestJudgeCriteria::test_clarity_criterion", "type": "Coroutine", "lineno": 578}]}, {"nodeid": "tests/evaluation/test_llm_as_judge.py", "outcome": "passed", "result": [{"nodeid": "tests/evaluation/test_llm_as_judge.py::TestLLMJudgeAccuracy", "type": "Class"}, {"nodeid": "tests/evaluation/test_llm_as_judge.py::TestJudgeConsistency", "type": "Class"}, {"nodeid": "tests/evaluation/test_llm_as_judge.py::TestJudgeDiscrimination", "type": "Class"}, {"nodeid": "tests/evaluation/test_llm_as_judge.py::TestJudgePairwiseComparison", "type": "Class"}, {"nodeid": "tests/evaluation/test_llm_as_judge.py::TestJudgeWithGroundTruth", "type": "Class"}, {"nodeid": "tests/evaluation/test_llm_as_judge.py::TestJudgeThresholds", "type": "Class"}, {"nodeid": "tests/evaluation/test_llm_as_judge.py::TestJudgeCriteria", "type": "Class"}]}, {"nodeid": "tests/evaluation/test_retrieval_metrics.py::TestContextPrecision", "outcome": "passed", "result": [{"nodeid": "tests/evaluation/test_retrieval_metrics.py::TestContextPrecision::test_precision_at_1_on_sample_queries", "type": "Coroutine", "lineno": 27}, {"nodeid": "tests/evaluation/test_retrieval_metrics.py::TestContextPrecision::test_precision_at_k", "type": "Coroutine", "lineno": 67}, {"nodeid": "tests/evaluation/test_retrieval_metrics.py::TestContextPrecision::test_precision_on_employment_queries", "type": "Coroutine", "lineno": 116}]}, {"nodeid": "tests/evaluation/test_retrieval_metrics.py::TestContextRecall", "outcome": "passed", "result": [{"nodeid": "tests/evaluation/test_retrieval_metrics.py::TestContextRecall::test_recall_on_ground_truth", "type": "Coroutine", "lineno": 163}]}, {"nodeid": "tests/evaluation/test_retrieval_metrics.py::TestMeanReciprocalRank", "outcome": "passed", "result": [{"nodeid": "tests/evaluation/test_retrieval_metrics.py::TestMeanReciprocalRank::test_mrr_calculation", "type": "Coroutine", "lineno": 217}]}, {"nodeid": "tests/evaluation/test_retrieval_metrics.py::TestHybridVsVectorOnly", "outcome": "passed", "result": [{"nodeid": "tests/evaluation/test_retrieval_metrics.py::TestHybridVsVectorOnly::test_hybrid_improves_over_vector_only", "type": "Coroutine", "lineno": 264}]}, {"nodeid": "tests/evaluation/test_retrieval_metrics.py::TestReranking", "outcome": "passed", "result": [{"nodeid": "tests/evaluation/test_retrieval_metrics.py::TestReranking::test_reranking_improves_precision", "type": "Coroutine", "lineno": 307}]}, {"nodeid": "tests/evaluation/test_retrieval_metrics.py::TestConfidenceThreshold", "outcome": "passed", "result": [{"nodeid": "tests/evaluation/test_retrieval_metrics.py::TestConfidenceThreshold::test_confidence_threshold_filters_low_quality", "type": "Coroutine", "lineno": 360}, {"nodeid": "tests/evaluation/test_retrieval_metrics.py::TestConfidenceThreshold::test_validate_default_threshold", "type": "Coroutine", "lineno": 387}]}, {"nodeid": "tests/evaluation/test_retrieval_metrics.py::TestRetrievalLatency", "outcome": "passed", "result": [{"nodeid": "tests/evaluation/test_retrieval_metrics.py::TestRetrievalLatency::test_retrieval_latency_under_2_seconds", "type": "Coroutine", "lineno": 426}]}, {"nodeid": "tests/evaluation/test_retrieval_metrics.py", "outcome": "passed", "result": [{"nodeid": "tests/evaluation/test_retrieval_metrics.py::TestContextPrecision", "type": "Class"}, {"nodeid": "tests/evaluation/test_retrieval_metrics.py::TestContextRecall", "type": "Class"}, {"nodeid": "tests/evaluation/test_retrieval_metrics.py::TestMeanReciprocalRank", "type": "Class"}, {"nodeid": "tests/evaluation/test_retrieval_metrics.py::TestHybridVsVectorOnly", "type": "Class"}, {"nodeid": "tests/evaluation/test_retrieval_metrics.py::TestReranking", "type": "Class"}, {"nodeid": "tests/evaluation/test_retrieval_metrics.py::TestConfidenceThreshold", "type": "Class"}, {"nodeid": "tests/evaluation/test_retrieval_metrics.py::TestRetrievalLatency", "type": "Class"}]}, {"nodeid": "tests/evaluation", "outcome": "passed", "result": [{"nodeid": "tests/evaluation/test_generation_metrics.py", "type": "Module"}, {"nodeid": "tests/evaluation/test_llm_as_judge.py", "type": "Module"}, {"nodeid": "tests/evaluation/test_retrieval_metrics.py", "type": "Module"}]}], "tests": [{"nodeid": "tests/evaluation/test_generation_metrics.py::TestBERTScoreEvaluation::test_bertscore_on_generated_answers", "lineno": 25, "outcome": "passed", "keywords": ["test_bertscore_on_generated_answers", "asyncio", "pytestmark", "TestBERTScoreEvaluation", "requires_llm", "slow", "evaluation", "test_generation_metrics.py", "tests", "app", "", "requires_db"], "setup": {"duration": 95.3058745849994, "outcome": "passed"}}]}